{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 503,
   "id": "3ca1e364",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import necessarily libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import time\n",
    "from datasketch import MinHash, MinHashLSHForest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 504,
   "id": "a59a1d4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Preprocess will split a string of text into individual tokens/shingles based on whitespace.\n",
    "def preprocess(text):\n",
    "    text = re.sub(r'[^\\w\\s]','',text)\n",
    "    tokens = text.lower()\n",
    "    tokens = tokens.split()\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f6c9f46",
   "metadata": {},
   "source": [
    "# First Layer Locallity Sensitive Hashing\n",
    "https://www.learndatasci.com/tutorials/building-recommendation-engine-locality-sensitive-hashing-lsh-python/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 505,
   "id": "29a6e21b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Number of Permutations\n",
    "permutations = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 506,
   "id": "2a2eccc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_forest(data, perms):\n",
    "    start_time = time.time()\n",
    "    \n",
    "    minhash = []\n",
    "    \n",
    "    for text in data['text']:\n",
    "        tokens = preprocess(text)\n",
    "        m = MinHash(num_perm=perms)\n",
    "        for s in tokens:\n",
    "            m.update(s.encode('utf8'))\n",
    "        minhash.append(m)\n",
    "        \n",
    "    forest = MinHashLSHForest(num_perm=perms)\n",
    "    \n",
    "    for i,m in enumerate(minhash):\n",
    "        forest.add(i,m)\n",
    "        \n",
    "    forest.index()\n",
    "    \n",
    "    print('It took %s seconds to build forest.' %(time.time()-start_time))\n",
    "    \n",
    "    return forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 507,
   "id": "0a8c2d22",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(text, database, perms, num_results, forest):\n",
    "    start_time = time.time()\n",
    "    \n",
    "    tokens = preprocess(text)\n",
    "    m = MinHash(num_perm=perms)\n",
    "    for s in tokens:\n",
    "        m.update(s.encode('utf8'))\n",
    "        \n",
    "    idx_array = np.array(forest.query(m, num_results))\n",
    "    if len(idx_array) == 0:\n",
    "        return None # if your query is empty, return none\n",
    "    \n",
    "    result = database.iloc[idx_array]['SpamTerms']\n",
    "    \n",
    "    print('It took %s seconds to query forest.' %(time.time()-start_time))\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "288ad73e",
   "metadata": {},
   "source": [
    "# Upload the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 508,
   "id": "ae2c9b4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "db = pd.read_csv('/Users/ibrahim/Desktop/TCN/FinalProject/Dataset/SpamTerms.csv')#Please change this to your folder location\n",
    "db['text']= db['SpamTerms']\n",
    "#print(db)\n",
    "#sep='|', names=m_cols , encoding='latin-1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 509,
   "id": "05500e14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   SpamTerms                     text\n",
      "0                    All-new                  All-new\n",
      "1                       Boss                     Boss\n",
      "2               Don’t delete             Don’t delete\n",
      "3        Drastically reduced      Drastically reduced\n",
      "4             Exclusive deal           Exclusive deal\n",
      "..                       ...                      ...\n",
      "566   You have been selected   You have been selected\n",
      "567  You have been selected!  You have been selected!\n",
      "568         You’re a Winner!         You’re a Winner!\n",
      "569    You’ve been selected!    You’ve been selected!\n",
      "570              Your income              Your income\n",
      "\n",
      "[571 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "db=db.head(573)#Read the number of rows you would like from the train dataset\n",
    "print(db)#404289"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 510,
   "id": "bd05e842",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It took 0.2173469066619873 seconds to build forest.\n"
     ]
    }
   ],
   "source": [
    "forest = get_forest(db, permutations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 576,
   "id": "7f1fa7c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It took 0.0021009445190429688 seconds to query forest.\n",
      "\n",
      " Top Recommendation(s) is(are) \n",
      " 555        Win\n",
      "556    Win big\n",
      "Name: SpamTerms, dtype: object\n"
     ]
    }
   ],
   "source": [
    "num_recommendations = 100 #We would like to get the best 100 candidates\n",
    "query = \"SIX chances to win CASH! From 100 to 20,000 pounds txt> CSH11 and send to 87575. Cost 150p/day, 6days, 16+ TsandCs apply Reply HL 4 info\"\n",
    "result = predict(query, db, permutations, num_recommendations, forest)\n",
    "print('\\n Top Recommendation(s) is(are) \\n', result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 577,
   "id": "22be0ba0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a list of candiates to be taken to the next layer which is Cosine Simialrity\n",
    "candidates=[]\n",
    "candidates=result.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 578,
   "id": "d01921d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Win' 'Win big']\n"
     ]
    }
   ],
   "source": [
    "print(candidates)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52fb23f8",
   "metadata": {},
   "source": [
    "# Second Layer BERT Vectors+Cosine Similarity\n",
    "https://www.analyticsvidhya.com/blog/2020/08/top-4-sentence-embedding-techniques-using-python/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 579,
   "id": "401ed70e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tokenize the candidates to be used in Bert Vector model\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "tokenized_sent = []\n",
    "for s in candidates:\n",
    "    tokenized_sent.append(word_tokenize(s.lower()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 580,
   "id": "0d651e97",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/ibrahim/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "from nltk.tokenize import word_tokenize\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 581,
   "id": "a772c276",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine(u, v):#Cosine Similarity Calculation\n",
    "    return np.dot(u, v) / (np.linalg.norm(u) * np.linalg.norm(v))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 582,
   "id": "a40f0205",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer #Vectorize the sentences using bert\n",
    "sbert_model = SentenceTransformer('bert-base-nli-mean-tokens')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 583,
   "id": "bb8a827a",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_embeddings = sbert_model.encode(candidates)\n",
    "\n",
    "#print('Sample BERT embedding vector - length', len(sentence_embeddings[0]))\n",
    "#print('Sample BERT embedding vector - note includes negative values', sentence_embeddings[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 584,
   "id": "a47af791",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_vec = sbert_model.encode([query])[0]#Vectorize the query using bert "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 585,
   "id": "023deab3",
   "metadata": {},
   "outputs": [],
   "source": [
    "Cosine_Candidates={} #This dicitonary to get the candidates and thier cosine similarity\n",
    "for sent in candidates:\n",
    "    sim = cosine(query_vec, sbert_model.encode([sent])[0])\n",
    "    #print(\"Sentence = \", sent, \"; similarity = \", sim)\n",
    "    Cosine_Candidates[sent]=[sim]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 586,
   "id": "56ffa99d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Win big': [0.29142788], 'Win': [0.1522886]}\n"
     ]
    }
   ],
   "source": [
    "import operator#To sort the candidates from the highest to the lowest\n",
    "sorted_d = dict(sorted(Cosine_Candidates.items(), key=operator.itemgetter(1),reverse=True))\n",
    "print(sorted_d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 587,
   "id": "5a16b8b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "NW_Candidates2=[] #The spam terms that has 85% similarity only\n",
    "for i in sorted_d:\n",
    "    if sorted_d[i] >= [0.0001]:\n",
    "        NW_Candidates2.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 588,
   "id": "5a9c1fad",
   "metadata": {},
   "outputs": [],
   "source": [
    "#sort the dictionary in descending way and get the best 10 possible similar spam\n",
    "NW_Candidates=NW_Candidates2[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 589,
   "id": "128240ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The SMS Text is classified as spam and the following spam terms detected\n",
      " ['Win big', 'Win']\n"
     ]
    }
   ],
   "source": [
    "print(\"The SMS Text is classified as spam and the following spam terms detected\\n\",NW_Candidates)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
